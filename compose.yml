services:
  web:
    image: ghcr.io/schaurian/schautrack:latest
    env_file: .env
    ports:
      - "3000:3000"
    depends_on:
      db:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "wget", "-q", "--spider", "http://localhost:3000/api/health"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 10s
    restart: unless-stopped

  db:
    image: postgres:18-alpine
    env_file: .env
    environment:
      - TZ=UTC
      - PGTZ=UTC
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U $POSTGRES_USER -d $POSTGRES_DB"]
      interval: 5s
      timeout: 3s
      retries: 5
      start_period: 5s
    volumes:
      - db-data:/var/lib/postgresql/data
    restart: unless-stopped

  # ============================================================================
  # OLLAMA (Optional) - Local AI for calorie estimation
  # ============================================================================
  # Uncomment to enable. Requires a GPU with sufficient VRAM.
  # CPU inference is possible but very slow and not recommended.
  # Set AI_PROVIDER=ollama and AI_MODEL=gemma3:12b in .env to use.
  #
  # ollama:
  #   image: ollama/ollama:latest
  #   ports:
  #     - "11434:11434"
  #   volumes:
  #     - ollama-data:/root/.ollama
  #   healthcheck:
  #     test: ["CMD", "ollama", "list"]
  #     interval: 10s
  #     timeout: 5s
  #     retries: 5
  #   restart: unless-stopped
  #   deploy:
  #     resources:
  #       reservations:
  #         devices:
  #           - driver: nvidia
  #             count: 1
  #             capabilities: [gpu]
  #
  # ollama-pull:
  #   image: ollama/ollama:latest
  #   depends_on:
  #     ollama:
  #       condition: service_healthy
  #   entrypoint: ["sh", "-c", "ollama pull ${AI_MODEL:-gemma3:12b}"]
  #   environment:
  #     - OLLAMA_HOST=ollama:11434
  #   env_file: .env
  #   restart: "no"

volumes:
  db-data:
  # ollama-data: